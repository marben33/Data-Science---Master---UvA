{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><span style=\"font-size:24px;\"><b> Physical Activity Recognition Group 15 - round 1 </b></span></center>\n\n\n<h1 id=introduction><br><span style=\"font-size: 20px;\"><b> Introduction to the project </b> </span></h1>\n\n<span style=\"font-size: 16px;\"> In this notebook we build a classifier algorithm that recognizes different types of physical movement and static postures (e.g. walking, sitting, standing up) based upon measurements from a smartphone accelerometer and a gyroscope. To train algorithm we use data provided the competition hosts Daniel van der Meer, Joost van Kordelaar, Dave Leitritz and Raoul Grasman (2023). This data was collected during experiments where a group of 30 participants had to complete a sequence of several movements and postures and measurements were taken by a smartphone strapped to their body from beginning to end of the sequence. This includes transitions periods such as sitting to standing. The accelerometer and a gyroscope data comes in the from of a constant rate of 50Hz measurments of three different axes.</span>\n    \n<span style=\"font-size: 16px;\"> Because one sequence of measurments contains multiple movements and postures, the total sequence is divided up in sections of sampled epochs, each of which has to be labeld with a movement or posture. This is the eventual output of the algorithm. In this notebook we show our way of tackling this problem, from loading in the data, to feature selection, to model creation and selection and finnaly arrive with a set of predictions which we submit as part of the kaggle competition. </span>\n\n<span style=\"font-size: 20px;\"><b> Structure of the notebook </b> </span>\n* R-setup\n* Importing data\n* Pre-processing data\n* Feature creation\n* Model creation\n* Model selection\n* Model prediction\n\n<h1 id=rsetup><span style=\"font-size: 20px;\"><b> Setting up R </b> </span></h1>\n\nWe load in packages, turn off warnings, and copy all files to our working directory","metadata":{}},{"cell_type":"code","source":"# load in packages\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(caret))\n\n\n# handle warnings\noptions(warn=-1)    # warnings off\n# options(warn=0)   # warnings on\n\n.... = NA\n\n# Copy all files to the working directory\nsystem(paste0(\"cp -r \", list.files(\"../input\", pattern = \"recognition\", full.names=TRUE), \"/* ./\"))","metadata":{"_uuid":"93de3694eaabf3b22b62e4876069a9cf8f92fb7e","_execution_state":"idle","execution":{"iopub.status.busy":"2023-09-28T09:55:11.548831Z","iopub.execute_input":"2023-09-28T09:55:11.551103Z","iopub.status.idle":"2023-09-28T09:55:18.071901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span style=\"font-size: 20px;\"><b>  Reading in the labels data </b></span></h1>\n\nIn this section we will import and structure data related to the activity labels (dependent variables):\n\n* The dependent variables ential physical activities, encompassing 12 labels. These labels represent activities such as walking, lying down, standing, and sitting. Some labels also capture transitions between these states, like moving from standing to sitting. Furthermore, we differentiate between walking downstairs and upstairs.\n\n* The goal is to develop a predictive model accurately forecasting these physical activity labels. To achieve this, we will leverage a collection of labeled files, which serve as our training data. Subsequently, we will assess the model's generalization ability by applying it to a set of unlabeled test data. We will evaluate its performance by constructing a confusion matrix, which will help us compare our predicted labels with the actual labels from the test data.\n\n<br><span style=\"font-size: 16px;\"><b> We import activity labels and convert numeric activity labels into lettered labels. </b></span>","metadata":{"_uuid":"968c9a784fa2d6ebd271ccb31a9c5cdc41175f25"}},{"cell_type":"code","source":"# load in activity lables\nact_labels <- read_delim(\"activity_labels.txt\",\n                         \" \",\n                         col_names = FALSE, \n                         trim_ws = TRUE,\n                         show_col_types = FALSE) \n\nact_labels <- act_labels %>% select(X1, X2)\n\n# load in labels\nlabels <- read_delim(\"./RawData/Train/labels_train.txt\",\n                     \" \",\n                     col_names = FALSE,\n                     show_col_types = FALSE)\ncolnames(labels) <- c('trial', 'userid', 'activity', 'start', 'end')\n\n# swap out number label for word label\nlabels <- labels %>% mutate(activity = act_labels$X2[activity])\n\nhead(labels)","metadata":{"_uuid":"f8fb07fb5ea7520789d0fb55afe4bfc555083bb6","execution":{"iopub.status.busy":"2023-09-28T09:55:21.705131Z","iopub.execute_input":"2023-09-28T09:55:21.748957Z","iopub.status.idle":"2023-09-28T09:55:22.163494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 16px;\"><b> We add a new column of lists that contain a sequence of sample id's. </b></span>\n    \nA sampleID essentially a description of a moment of measurement, or a place in the sequence of measurements. The lists we make in the next cell contain all the sampleIDs from the start of an activity label to the end of the activity label. In the print statement you can see the length of the lists, or rather the number of sampleIDs they contain.","metadata":{"_uuid":"999173cbb57a05233f5b70f9c89a4eee6efee205"}},{"cell_type":"code","source":"# Add the sequence start:end to each row in a list.\n# The result is a nested table:\nsample_labels_nested <-\n    labels %>% \n    rowwise() %>% # do next operation(s) rowwise\n    mutate(sampleid = list(start:end)) %>%\n    ungroup()\n\nprint(sample_labels_nested)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:55:24.865294Z","iopub.execute_input":"2023-09-28T09:55:24.866962Z","iopub.status.idle":"2023-09-28T09:55:24.951381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 16px;\"><b> We unnest the lists of sample id's </b></span>\n\nLists that are nested into a table are complicated to access. Therefore we unnest the lists into one big table we can easily use later.","metadata":{}},{"cell_type":"code","source":"# Unnest the nested tabel\nsample_labels <-\n    sample_labels_nested %>% \n\n    # Rows are segments, we need to keep track of different segements\n    mutate(segment = row_number() ) %>% \n\n    # Expand the data frame to one sample per row\n    unnest(cols = c(sampleid)) %>% \n\n    # Remove columns we don't need anymore\n    select(-start, -end)\n\nprint(sample_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:55:29.547559Z","iopub.execute_input":"2023-09-28T09:55:29.548962Z","iopub.status.idle":"2023-09-28T09:55:30.010722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 16px;\"><b> The final labels table </b></span>\n\nThe labels table we created contains alot important information about our measurements. We have for each measurement, what trial was going on, who was being measured, what activity they were doing, at what point in the sequence of samples it came and the segment in which it came.\n_______________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"________________________________________________________________________________________________________\n<h1 id=functions><span style=\"font-size: 20px;\"><b> Feature extraction functions </b></span></h1>\n\nBefore we start extracting features from the data signals we define helper functions to help keep the code clean and simple. We definde two types of helper functions those related to time domain features and time frequency features. The difference between them is that time domain features relate to the actual values of the measurements, while time frequency features relate to spectral properties.\n\nThe data processing approach involved organizing the sample IDs into epochs, with each epoch containing 128 samples. Within each epoch, we extracted features from the data. The activity label assigned to each epoch was determined by identifying the most frequently occurring activity during that specific epoch.\n\n<span style=\"font-size: 16px;\"><b> Time domain features </b></span>\n\n*Description of created time domain features*\n\n* **Most common value**: Returns the value that has the highest occurrence count within a given set of values (it is also known as the mode in statistics).\n\n* **Lagged correlations**: It measures the correlation between two time series variables with a time delay or lag between them. In other words, lagged correlations explore how one variable is related to another when there is a temporal offset between their observations (Grasman, 2018).\n\n* **Root mean square (RMS)**: It is used to find the average value of a set of values or the \"typical\" magnitude of a varying quantity. The RMS is particularly useful when dealing with values that fluctuate, such as time-varying signals or data. First we have to square the values, calculate their mean and take their square root.\n\n* **Scaled inner product (SIP)**: It measures two vectors' similarity in space, and scaling involves multiplying this similarity measure by a constant factor (Grasman, 2018).","metadata":{"_uuid":"903d9aabb30f663db30ee64963d4165f6ad8a14f"}},{"cell_type":"code","source":"## Feature functions\n\n# Most common value (base)\nmost_common_value = function(x) {\n    counts = table(x, useNA='no')\n    most_frequent = which.max(counts)\n    return(names(most_frequent))\n}\n\n# Lagged correlations (base)\nlagged_cor <- function(x, y=x, lag=0) {\n    # compute correlation between x and a time shifted y\n    r_lagged = cor(x, dplyr::lag(y, lag), use='pairwise')\n    r_lagged = ifelse (is.na(r_lagged), 0, r_lagged)\n    return(r_lagged)\n}\n\n# Root mean square (internet)\nrms <- function(x){\n  sqrt((1/length(x)*(sum(x^2))))\n}\n\n# Scaled inner product (Grasman, R. (2018))\nSIP = function(x1, x2) {\n    sip = x1 %*% x2 / sqrt(sum(x1^2) * sum(x2^2))\n    sip = as.double(sip)\n    return(sip)\n}","metadata":{"_uuid":"6cec2d0fa3e99e2d924c24ac55c5d462ae666707","execution":{"iopub.status.busy":"2023-09-28T09:55:34.870292Z","iopub.execute_input":"2023-09-28T09:55:34.871818Z","iopub.status.idle":"2023-09-28T09:55:34.890737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 16px;\"><b> Frequency domain features </b></span>\n\n*Description of created frequency domain features*\n\n* **Entropy**: It measures the amount of uncertainty in a set of data or a probability distribution, therefore quantifies the unpredictability of the outcomes in a dataset. Overall, it returns the amount of information present (Grasman, 2018). \n\n* **Energy**: Returns the “total power” of a signal in n samples (Grasman, 2018).\n\n* **Spectral peak**: Returns a distinct local maximum in the spectral density or power spectrum of a signal. The spectral density or power spectrum is a representation of how the power or energy of a signal is distributed across different frequencies.\n\n* **Spectral entropy**: Returns the etropy of a spectrum, quantifying the amount of spectral information. It measures how uniformly the power or energy of a signal is distributed across different frequency components (randomness of frequency components in a signal) (Grasman, 2018).\n\n* **Spectral mean and standard deviation (SD)**: Mean and SD of a spectrum (Grasman, 2018). It summarizes the characteristics of a signal's frequency spectrum.\n\n* **Spectral mode, median, kurtosis (from Group 16)**: Most common spectral frequency, median spectral frequency and the sharpness of the spectral distribution.","metadata":{}},{"cell_type":"code","source":"# Entropy (Grasman, R. (2018))\nentropy  <- function(x, nbreaks = nclass.Sturges(x)) {\n    r = range(x)\n    x_binned = findInterval(x, seq(r[1], r[2], len = nbreaks))\n    h = tabulate(x_binned, nbins = nbreaks)\n    p = h/sum(h)\n    entropy = -sum(p[p>0] * log(p[p>0]))\n    return(entropy)\n}\n\n# Energy (Grasman, R. (2018))\nenergy = function(x) {\n  sum(x^2)\n}\n\n# Spectral peak (Grasman, R. (2018))\nspec_peak = function(x) {\n  spec = spectrum(x, plot = FALSE)\n  return(spec$freq[which.max(spec$spec)])\n}\n\n# Spectral entropy (Grasman, R. (2018))\nspec_entr = function(x) {\n    spec = spectrum(x, plot = FALSE)$spec\n    entropy(spec)\n}\n\n# Spectral mean (Grasman, R. (2018))\nspec_avg <- function(x) {\n  spec = spectrum(x, plot = F)\n  df = spec$freq[2] - spec$freq[1]\n  return(sum(spec$freq * spec$spec * df))\n}\n\n# Spectral standard deviation (Grasman, R. (2018))\nspec_sd <- function(x) {\n    spec = spectrum(x, log = 'n', plot = FALSE)$spec\n    freq = spectrum(x, log = 'n', plot = FALSE)$freq\n    df = freq[2] - freq[1]\n    \n    return(sqrt(sum((freq - mean(x))^2 * spec * df)))\n}\n\n# Spectral Mode, Median, and Kurtosis\n# in analogy from Grasman (2018) but taken from Group 16\nspec_mode <- function(x) {\n    spec_features <- spectrum(x, plot = FALSE)\n    mode <- max(spec_features$spec)\n    return(mode)\n}\n\nspec_median <- function(x) {\n    spec_features <- spectrum(x, plot = FALSE)\n    median <- median(spec_features$spec)\n    return(median)\n}\n\nspec_kurt <- function(x) {\n    spec_features <- spectrum(x, plot = FALSE)\n    kurt <- e1071::kurtosis(spec_features$spec)\n    return(kurt)\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:55:40.340734Z","iopub.execute_input":"2023-09-28T09:55:40.342181Z","iopub.status.idle":"2023-09-28T09:55:40.369282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Stolen features </b>\n\nBecause all good artists steal, we looked at some submissions from previous iterations of this competition. Michael Ogwuru, Wesley Korff and \nElisa Mens (group 12) were the winners of last years competition came up with a feature that compares the mean of intervals within an epoch to that of the overal variable. For this they used the following helper function.\n\nWe also take the idea of using derivatives from Daphne Jonkers Both and Noah Hatakeyama (last years group 10)\n\nWith the use of derivates they compute twelve total features:\n- mean, sd, skew, and kurtosis of linear jerk\n- mean, sd, skew, and kurtosis of angular acceleration\n- mean, sd, skew, and kurtosis of angular jerk\n\nWe won't use the last bit, since that would require significant changes to our code.","metadata":{}},{"cell_type":"code","source":"# from last years group 12\n\n# The difference in mean of epoch with overall mean \naverage_within <- function(x){\n    interval <- as.numeric()\n    interval <- c(x[1:9] %>% mean(),\n                  x[10:19] %>% mean(),\n                  x[20:29] %>% mean(),\n                  x[30:39] %>% mean(),\n                  x[40:49] %>% mean(),\n                  x[50:59] %>% mean(),\n                  x[60:69] %>% mean(),\n                  x[70:79] %>% mean(),\n                  x[80:89] %>% mean(),\n                  x[90:99] %>% mean(),\n                  x[100:109] %>% mean(),\n                  x[110:119] %>% mean(),\n                  x[120:128] %>% mean())\n    average_interval <- as.numeric()\n    for (i in 1:13){\n        average_interval[i] <- interval[i] - mean(x)\n    }\n    return(sum(average_interval))\n}\n\n# from last years group 10\n# Computes first derivative, applies the function specified in `type` and returns it\nsingle_deriv <- function(x, type=mean, ...) {\n    x %>% \n    diff %>% \n    type(., ...) %>% \n    return\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:55:43.705655Z","iopub.execute_input":"2023-09-28T09:55:43.708736Z","iopub.status.idle":"2023-09-28T09:55:43.727090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"________________________________________________________________________________________________________\n<h1 id=combi><span style=\"font-size: 20px;\"><b> Combining the feature extration functions </b></span></h1>\n\nNow that we have put in place all the helper functions that we need we can define our big feature extraction function. It's a big function with several steps to it. It's going to take as input a file name and the sample labels table that we created previously. From the file name it's going extract the username and trial id and import the sensor signals from the file. We then create a column which ranks the order of appaerance of the sensor measurements. This is the sampleID we also have stored in labels. We now have the required information to join in the label activity data from our sample_labels table.\n\nNext up our function is going split our data into epochs of 128 sample and compute our features per epoch. One epoch is a timeframe of 128 samples, which corresponds to 2.56 seconds.\n\nWe extract the following list of features for each spatial axis (remember that the gyroscope and accellorater record data on three axes).\n\n- mean \n- most common values \n- median\n- range\n- 25th quantile\n- 75th quantile\n- skewness\n- kurtosis\n- power\n- root mean square\n- number of values below mean\n- entropy\n- energy\n- number of positive features\n- spectral peak\n- spectral energy\n- spectral mean\n- spectral standard deviation\n- spectral standard mode\n- spectral standard median\n- spectral standard kurtosis\n- auto_correlation with lags 1 and 2\n\nFurthermore several features are extracted based on the relationship between the axes\n\n- lagged_correlations with lags 1 and 2 between all features\n- mean difference between axes\n- scaled inner product\n\nLastly we steal some features from groups that competed in lasts competition\nWe steal from group 1 (Pander Abbing, Meike Waaijers and Paulo Ortiz) and from group 12 (Michael Ogwuru, Wesley Korff and \nElisa Mens)\n\n- signal magnitude area\n- width to height ratio\n- average resultant acceleration\n- difference in mean of intervals within epoch with overall mean\n\nWe also compute the most common activity label in each epoch, keep track of userid, experimentid and compute the number of samples.\n\n\n\nAt the end of the function it returns the table with all the features it extracted.","metadata":{"_uuid":"70f82939d0025cd90b72ea9b46b23c9a110f4f38"}},{"cell_type":"code","source":"extractDomainFeatures <- function(filename, sample_labels) {\n    \n    # extract user and experimental run ID's from file name\n    username = gsub(\".+user(\\\\d+).+\", \"\\\\1\", filename) %>% as.numeric()\n    expname  = gsub( \".+exp(\\\\d+).+\", \"\\\\1\", filename) %>% as.numeric()\n    \n    # import the sensor signals from the file\n    user01 <- read_delim(filename, \" \", col_names = F, progress = TRUE, \n                 col_types = \"ddd\")\n    \n    # merge signals with labels \n    user_df <- \n        data.frame(userid = username, trial = expname, user01) %>%\n        mutate(sampleid = 0:(nrow(user01)-1) ) %>%\n        left_join(sample_labels, by = c('userid','trial','sampleid')) \n\n    \n    # split in epochs of 128 samples and compute features per epoch\n    usertimedom <-  user_df %>%\n    \n          # add an epoch ID variable (on epoch = 2.56 sec)\n          mutate(epoch = sampleid %/% 128) %>% \n\n          # extract statistical features from each epoch\n          group_by(epoch) %>%\n          summarise(\n              \n            # keep track of user and experiment information\n            user_id = username, \n            exp_id = expname,   \n              \n            # epoch's activity labels and start sample\n            activity = most_common_value(c(\"-\", activity)),\n            sampleid = sampleid[1],\n              \n            # features\n            # means\n            m1 = mean(X1), \n            m2 = mean(X2),\n            m3 = mean(X3),\n              \n            # most common values\n            most_common_value1 = as.numeric(most_common_value(X1)),\n            most_common_value2 = as.numeric(most_common_value(X2)),\n            most_common_value3 = as.numeric(most_common_value(X3)),\n\n            # medians\n            median1 = median(X1),\n            median2 = median(X2),\n            median3 = median(X3),\n\n            # ranges\n            range1 = max(X1) - min (X1),\n            range2 = max(X2) - min (X2),\n            range3 = max(X3) - min (X3),\n\n            # standard deviations\n            sd1 = sd(X1),\n            sd2 = sd(X2),\n            sd3 = sd(X3),\n\n            # quantiles (25% & 75%)\n            q1_25 = quantile(X1, .25),\n            q2_25 = quantile(X2, .25),\n            q3_25 = quantile(X3, .25),\n            q1_75 = quantile(X1, .75),\n            q2_75 = quantile(X2, .75),\n            q3_75 = quantile(X3, .75),\n\n            # skewness\n            skew1 = e1071::skewness(X1),\n            skew2 = e1071::skewness(X2),\n            skew3 = e1071::skewness(X3),\n\n            # kurtosis\n            kurt1 = e1071::kurtosis(X1),\n            kurt2 = e1071::kurtosis(X2),\n            kurt3 = e1071::kurtosis(X3),\n\n            # correlations\n            cor12 = cor(X1, X2),\n            cor13 = cor(X1, X3), \n            cor23 = cor(X2, X3),  \n            \n            # lagged correlations\n            AR1_1 = lagged_cor(X1, lag=1),\n            AR1_2 = lagged_cor(X1, lag=2),\n            AR2_1 = lagged_cor(X2, lag=1),\n            AR2_2 = lagged_cor(X2, lag=2), \n            AR3_1 = lagged_cor(X3, lag=1),\n            AR3_2 = lagged_cor(X3, lag=2),\n            AR12_1 = lagged_cor(X1, X2, lag=1),\n            AR12_2 = lagged_cor(X1, X2, lag=2),\n            AR13_1 = lagged_cor(X1, X3, lag=1),\n            AR13_2 = lagged_cor(X1, X3, lag=2),\n            AR23_1 = lagged_cor(X2, X3, lag=1),\n            AR23_2 = lagged_cor(X2, X3, lag=2),\n            \n            # power\n            power_1 = mean(X1^2),\n            power_2 = mean(X2^2),\n            power_3 = mean(X3^2),\n            \n            # root mean square\n            rms1 = rms(X1),\n            rms2 = rms(X2),\n            rms3 = rms(X3),\n              \n            # mean differences between axes\n            mdif1_2 = m1 - m2, \n            mdif1_3 = m1 - m3,\n            mdif2_3 = m2 - m3,\n              \n            # number of values below mean\n            nVBM_1 = sum(X1 < mean(X1)),\n            nVBM_2 = sum(X2 < mean(X2)),\n            nVBM_3 = sum(X3 < mean(X3)),\n              \n            # scaled inner product\n            SIP12 = SIP(X1, X2),\n            SIP13 = SIP(X1, X3),\n            SIP23 = SIP(X2, X3),\n              \n            # entropy\n            entropy_1 = entropy(X1),\n            entropy_2 = entropy(X2),\n            entropy_3 = entropy(X3),\n              \n            # energy\n            energy_1 = energy(X1),\n            energy_2 = energy(X2),\n            energy_3 = energy(X3),\n              \n            # n of positive features\n            NPF_1 = sum(X1 > 0),\n            NPF_2 = sum(X2 > 0),\n            NPF_3 = sum(X3 > 0),\n              \n            # spectral peak\n            spec_peak_1 = spec_peak(X1),\n            spec_peak_2 = spec_peak(X2),\n            spec_peak_3 = spec_peak(X3),\n              \n            # spectral entropy\n            spec_entr_1 = spec_entr(X1),\n            spec_entr_2 = spec_entr(X2),\n            spec_entr_3 = spec_entr(X3),\n              \n            # spectral mean\n            spec_avg_1 = spec_avg(X1),\n            spec_avg_2 = spec_avg(X2),\n            spec_avg_3 = spec_avg(X3),\n              \n            # spectral standard deviation\n            spec_sd_1 = spec_sd(X1),\n            spec_sd_2 = spec_sd(X2),\n            spec_sd_3 = spec_sd(X3),\n              \n            # Spectral Mode\n            spec_mode_1 = spec_mode(X1),\n            spec_mode_2 = spec_mode(X2),\n            spec_mode_3 = spec_mode(X3),\n            \n            # Spectral Median\n            spec_median_1 = spec_median(X1),\n            spec_median_2 = spec_median(X2),\n            spec_median_3 = spec_median(X3),\n            \n            # Spectral Kurtosis\n            spec_kurt_1 = spec_kurt(X1),\n            spec_kurt_2 = spec_kurt(X2),\n            spec_kurt_3 = spec_kurt(X3),\n            \n            #average resultant acceleration (from last years group 1)\n            ARA = mean(sqrt(X1^2 + X2^2 + X3^2)),\n              \n            # signal magnitude area (from last years group 1)\n            SMA = sum(abs(X1) / 128) + sum(abs(X2) / 128) + sum(abs(X3) / 128),\n              \n            # Width to height ratio (from last years group 1)\n            whratio1 = X1[which.max(X1)] / length(sampleid),\n            whratio2 = X2[which.max(X2)] / length(sampleid),\n            whratio3 = X3[which.max(X3)] / length(sampleid),\n            \n            # Average comparison (from last years group 12)\n            average_within_x1 = average_within(X1),  \n            average_within_x2 = average_within(X2),\n            average_within_x3 = average_within(X3),\n              \n            # Derivative features # (from last years group 10)\n            deriv_mean_X1 = single_deriv(X1),\n            deriv_mean_X2 = single_deriv(X2),\n            deriv_mean_X3 = single_deriv(X3),\n            deriv_sd_X1 = single_deriv(X1, sd),\n            deriv_sd_X2 = single_deriv(X2, sd),\n            deriv_sd_X3 = single_deriv(X3, sd),\n            deriv_skew_X1 = single_deriv(X1, e1071::skewness),\n            deriv_skew_X2 = single_deriv(X2, e1071::skewness),\n            deriv_skew_X3 = single_deriv(X3, e1071::skewness),\n            deriv_kurt_X1 = single_deriv(X1, e1071::kurtosis),\n            deriv_kurt_X2 = single_deriv(X2, e1071::kurtosis),\n            deriv_kurt_X3 = single_deriv(X3, e1071::kurtosis),\n              \n            n_samples = n()  \n          ) \n    \n    usertimedom \n}","metadata":{"_uuid":"daccc2b4175580864f72a2012ef6d56ee7c0a883","execution":{"iopub.status.busy":"2023-09-28T09:55:53.017091Z","iopub.execute_input":"2023-09-28T09:55:53.018492Z","iopub.status.idle":"2023-09-28T09:55:53.035010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 16px;\"><b> Checking if the feature extration extraction function works </b></span>\n\nBefore we continue on with extracting features for all participants. We first want to test our function on the data of the first participant to see if it works.","metadata":{}},{"cell_type":"code","source":"# Check if function works for acceleration data\nfilename <- \"./RawData/Train/acc_exp01_user01.txt\"\nextractDomainFeatures(filename, sample_labels) %>% print()","metadata":{"_uuid":"d27ff61c2208bdd5366eac15153ad1086de930db","execution":{"iopub.status.busy":"2023-09-28T09:55:58.290962Z","iopub.execute_input":"2023-09-28T09:55:58.292828Z","iopub.status.idle":"2023-09-28T09:56:00.657775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if function works for gyro data\nfilename <- \"./RawData/Train/gyro_exp01_user01.txt\"\nextractDomainFeatures(filename, sample_labels) %>% print()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:56:02.929457Z","iopub.execute_input":"2023-09-28T09:56:02.930970Z","iopub.status.idle":"2023-09-28T09:56:04.772868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"________________________________________________________________________________________________________\n<h1 id=features><span style=\"font-size: 20px;\"><b> Extracting the features </h1></span></b>\n\nNow its time to put our function to use and extract features for all our data. We do this by first extracting the features for the acceleration data and gyroscope data seperately.\n\n<b> Training data for acceleration features </b>\n\nWe load in all the filenames, then using map_dfr run our feature extraction function on all files and then bind the tables together row wise. Finally we rename the variables to indicate that they originate from accelartion data.","metadata":{}},{"cell_type":"code","source":"# Acc features\nfilenames <- dir(\"./RawData/Train/\", \"^acc\", full.names = TRUE)\n\n# map_dfr runs `extractTimeDomainFeatures` on all elements in filenames \n# and binds results row wise\nmyData_acc <- map_dfr(filenames, extractDomainFeatures, sample_labels) \n\n# Label as Acc feats\nmyData_acc <- myData_acc %>% \n    rename_with(~paste0(\"AC_\", .x), 6:length(names(myData_acc)))\n\n# Check the result\nprint(myData_acc)","metadata":{"_uuid":"c2572d092b5b590a3f6dcc67313ff5f690beb471","execution":{"iopub.status.busy":"2023-09-28T09:56:12.479562Z","iopub.execute_input":"2023-09-28T09:56:12.480920Z","iopub.status.idle":"2023-09-28T09:57:21.183717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out distribution n_samples\nmyData_acc %>% group_by(AC_n_samples) %>% summarize(n = n())","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:57:37.241188Z","iopub.execute_input":"2023-09-28T09:57:37.242876Z","iopub.status.idle":"2023-09-28T09:57:37.285483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are some epochs which have fewer samples than 128\n# We delete them to train our models only on those observations,\n# which also provide a lot of data.\n\n# The overall number of observations has been\nmyData_acc %>% nrow()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:57:42.551414Z","iopub.execute_input":"2023-09-28T09:57:42.552969Z","iopub.status.idle":"2023-09-28T09:57:42.569190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So, this exclusion is not drastic.\nmyData_acc <- myData_acc %>% filter(AC_n_samples == 128)\nmyData_acc %>% select(AC_n_samples) %>% pull() %>% unique() %>% print()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:57:44.180819Z","iopub.execute_input":"2023-09-28T09:57:44.182226Z","iopub.status.idle":"2023-09-28T09:57:44.205978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finally, we remove this column, as its not a feature\nmyData_acc <- myData_acc %>% select(-AC_n_samples)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:57:46.153993Z","iopub.execute_input":"2023-09-28T09:57:46.155428Z","iopub.status.idle":"2023-09-28T09:57:46.169898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Training data for gyroscope features </b>\n\nWe repeat the same process for the gyroscope data.","metadata":{}},{"cell_type":"code","source":"# Gyro features\nfilenames <- dir(\"./RawData/Train/\", \"^gyro\", full.names = TRUE)\n\n# map_dfr runs `extractTimeDomainFeatures` on all elements in filenames \n# and binds results row wise\nmyData_gyro <- map_dfr(filenames, extractDomainFeatures, sample_labels) \n\n# Label as Acc feats\nmyData_gyro <- myData_gyro %>% \n    rename_with(~paste0(\"GY_\", .x), 6:length(names(myData_gyro)))\n\n# Check the result\nprint(myData_gyro)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:57:49.143418Z","iopub.execute_input":"2023-09-28T09:57:49.144814Z","iopub.status.idle":"2023-09-28T09:58:57.711288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out distribution n_samples\nmyData_gyro %>% group_by(GY_n_samples) %>% summarize(n = n())","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:08.595312Z","iopub.execute_input":"2023-09-28T09:59:08.596813Z","iopub.status.idle":"2023-09-28T09:59:08.636691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are some epochs which have fewer samples than 128\n# We delete them to train our models only on those observations,\n# which also provide a lot of data.\n\n# The overall number of observations has been\nmyData_gyro %>% nrow()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:11.602837Z","iopub.execute_input":"2023-09-28T09:59:11.604392Z","iopub.status.idle":"2023-09-28T09:59:11.620589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So, this exclusion is not drastic.\nmyData_gyro <- myData_gyro %>% filter(GY_n_samples == 128)\nmyData_gyro %>% select(GY_n_samples) %>% pull() %>% unique() %>% print()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:14.685841Z","iopub.execute_input":"2023-09-28T09:59:14.687429Z","iopub.status.idle":"2023-09-28T09:59:14.709247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finally, we remove this column, as its not a feature\nmyData_gyro <- myData_gyro %>% select(-GY_n_samples)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:16.880295Z","iopub.execute_input":"2023-09-28T09:59:16.881940Z","iopub.status.idle":"2023-09-28T09:59:16.897425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Merging the accelaration and gyroscope features </b>\n\nWe merge our feature tables by left joining the accelaration features and gyroscope features by user id, activity label, sampleid and epoch.","metadata":{}},{"cell_type":"code","source":"# Merge the two\ntrain_df <- myData_acc %>% \n    left_join(myData_gyro,\n              by = c(\"user_id\", \"exp_id\", \"activity\", \"sampleid\", \"epoch\")\n    )\n# Check\ntrain_df %>% select(c(1:8), c(85:87)) %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:19.330976Z","iopub.execute_input":"2023-09-28T09:59:19.332419Z","iopub.status.idle":"2023-09-28T09:59:19.377830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Checking if all participants were correctly loaded in </b>\n\nTo make sure we loaded in the data of all participants and didn't leave anyone out we print the unique user IDs and check how many participants are present.","metadata":{}},{"cell_type":"code","source":"# Check if other participants are also in there\ntrain_df %>% \n   filter(user_id == 30) %>% \n   select(c(1:8), c(85:87)) %>% \n   head(3)\n\nunique_users <- unique(train_df$user_id)\nprint(unique_users)\nprint(length(unique_users))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:22.228631Z","iopub.execute_input":"2023-09-28T09:59:22.230157Z","iopub.status.idle":"2023-09-28T09:59:22.286494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"________________________________________________________________________________________________________\n<h1 id=clean><span style=\"font-size: 20px;\"><b> Data cleaning </h1></span></b>\n\nHowever, our table is not ready for use just yet. We have to do some data cleaning before it's ready for use. Unlabelled epoch, near zero variation variables, highly correlated variables, highly correlated linear combinations are going to be removed.\n\n<b> Unlabelled Epochs (i.e. no activity label) </b>\n\nOne problem with our data is that there are epochs without an activity label. We have to remove those before we fit our models.","metadata":{}},{"cell_type":"code","source":"# How many observations per class?\ntrain_df %>% group_by(activity) %>% summarize(n = n())","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:27.056493Z","iopub.execute_input":"2023-09-28T09:59:27.058565Z","iopub.status.idle":"2023-09-28T09:59:27.088751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove unlabeled epochs\ntrain_df <- train_df %>% filter(activity != \"-\")\ntrain_df %>% group_by(activity) %>% summarize(n = n())\nnrow(train_df)\nncol(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:29.701821Z","iopub.execute_input":"2023-09-28T09:59:29.703534Z","iopub.status.idle":"2023-09-28T09:59:29.751687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Removing variables with near zero variation </b>\n\nSome features might not be useable in some of our models because they have near zero variation. We remove variables with with near zero variation.","metadata":{}},{"cell_type":"code","source":"## Near zero variation\nnzv_var = caret::nearZeroVar(train_df[,6:ncol(train_df)])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:34.614220Z","iopub.execute_input":"2023-09-28T09:59:34.615723Z","iopub.status.idle":"2023-09-28T09:59:38.094609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Removing multicollinearity (highly correlated variables) </b>\n\nVariables that are highly correlated with others don't any predictive value to our model and can cause problems with fitting. We remove highly correlated variables.","metadata":{}},{"cell_type":"code","source":"## Highly correlated variables\ncorr_var = train_df[,6:ncol(train_df)] %>%\n    cor(use = \"complete.obs\") %>%\n    caret::findCorrelation()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:44.211248Z","iopub.execute_input":"2023-09-28T09:59:44.212759Z","iopub.status.idle":"2023-09-28T09:59:44.474754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Removing highly correlated linear combinations </b>\n\nIt can also be the case that a combination of variables in feature set are able to nearly perfectly predict other features in our data set. Keeping those variables does not add any value to our models. We remove highly correlated linear combinations.\n\nAdditionally, all these 3 checks are going to be saved in a vector (\"to_be_removed\")","metadata":{}},{"cell_type":"code","source":"## Linear combinations\nlc_var = train_df[,6:ncol(train_df)] %>%\n    filter(complete.cases(.)) %>%\n    caret::findLinearCombos()\n\n# Save the columns of predictors to be removed\nto_be_removed = c(nzv_var, corr_var, lc_var$remove) + 5\nto_be_removed","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:46.797772Z","iopub.execute_input":"2023-09-28T09:59:46.799111Z","iopub.status.idle":"2023-09-28T09:59:46.971049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing all indicated predictors detected above\ntrain_df_3 = train_df[,-to_be_removed]\n\nhead(train_df_3)\n\ndim(train_df_3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:49.174665Z","iopub.execute_input":"2023-09-28T09:59:49.176101Z","iopub.status.idle":"2023-09-28T09:59:49.267981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before feature selection, we also remove the 4 non-features that are left:","metadata":{}},{"cell_type":"code","source":"train_df_3 <- train_df_3 %>% select(-c(epoch, user_id, exp_id, sampleid))\nhead(train_df_3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:52.405552Z","iopub.execute_input":"2023-09-28T09:59:52.406884Z","iopub.status.idle":"2023-09-28T09:59:52.494042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"________________________________________________________________________________________________________\n<h1 id=fselect><span style=\"font-size: 20px;\"><b> Feature selection </h1></span></b>\n\nTo prevent overfitting, we only want to use those features that are quite good at predicting the activities. We will therefore rank the features regarding their importance. Afterwards we will select the top X features (e.g. 10, 20, 30, 40, 50) and train our models on those feature subsets. Then we will do a kind of manual stepwise feature selection. The goal is to use as little features as possible while still maintaining a high accuracy.","metadata":{}},{"cell_type":"markdown","source":"<b> ANOVA Approach </b>\n\nTo rank the features regarding their importance we will run an ANOVA for each feature:\nfeature ~ activitiy\nBut we will do this in 5 subdataframes which each have a random 20% of the observations.\nThis way we get more reliabile estimates (F-values) about the importance for each features and whether this generalizes. After ranking the features in each of the 5 subdataframes regarding their F-values, we square the ranks to penalize lower rankings and then sum the squared rankings for each features across the 5 subdataframes. This yields the final ranking that we can then use for our stepwise inclusion during the model selection.","metadata":{}},{"cell_type":"code","source":"# Factorize activity\ntrain_df_3$activity <- factor(train_df_3$activity)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:57.378363Z","iopub.execute_input":"2023-09-28T09:59:57.380456Z","iopub.status.idle":"2023-09-28T09:59:57.397256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to calculate the F-value from an ANOVA\nget_pred_fval <- function(predictor, activity) {\n    # Perform an ANOVA with predictor and activity as factors\n    current_aov <- aov(predictor ~ activity)\n    \n    # Summarize the ANOVA results\n    result <- summary(current_aov)\n    \n    # Extract the F-value from the summary\n    f_value <- result[[1]]$F\n    f_value <- f_value[[1]]\n    \n    # Return the F-value\n    return(f_value)\n}\n\n# Perform an ANOVA on the entire dataset (train_df_3$AC_sd3)\nresult <- aov(train_df_3$AC_sd3 ~ train_df_3$activity, data = train_df_3)\n\n# Summarize the ANOVA results\nresult_summary <- summary(result)\n\n# Extract the F-value from the summary\nf_value <- result_summary[[1]]$F\nf_value <- f_value[[1]]\n\n# Print the F-value\nf_value\n\n# Print \"test\" as a marker\nprint(\"test\")\n\n# Call the get_pred_fval function for another predictor (train_df_3$AC_q1_25)\nget_pred_fval(train_df_3$AC_q1_25, train_df_3$activity)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T09:59:59.780764Z","iopub.execute_input":"2023-09-28T09:59:59.782824Z","iopub.status.idle":"2023-09-28T09:59:59.854869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an empty vector to store F-values\nf_values <- c()\n\n# Loop through predictor columns (starting from the second column)\nfor (pred_index in 2:ncol(train_df_3)) {\n    # Call the get_pred_fval function to calculate the F-value for each predictor\n    current_f <- get_pred_fval(train_df_3[[pred_index]], train_df_3$activity)\n    \n    # Store the F-value in the f_values vector\n    f_values[pred_index - 1] <- current_f\n}\n\n# Print the first few F-values (head)\nhead(f_values)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:03.220191Z","iopub.execute_input":"2023-09-28T10:00:03.221585Z","iopub.status.idle":"2023-09-28T10:00:03.626940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the column names of the original data frame (train_df_3)\npredictor <- names(train_df_3)\n\n# Exclude the first column (assumed to be the response variable)\npredictor <- predictor[-1]\n\n# Create a rank vector using square values to penalize lower rankings\nrank <- seq(1:length(f_values)) ^ 2\n\n# Create a new data frame df_concept\ndf_concept <- data.frame(\n    predictor,   # Column names (predictor variables)\n    f_values     # F-values calculated earlier\n)\n\n# Sort the data frame by F-values in descending order\nsorted_df <- df_concept[order(-df_concept$f_values), ]\n\n# Add a \"rank\" column to the sorted data frame\nsorted_df$rank <- rank\n\n# Select only the \"predictor\" and \"rank\" columns, excluding \"f_values\"\nsorted_df <- sorted_df %>% select(-f_values)\n\n# Print the first few rows of the sorted data frame\nhead(sorted_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:05.474769Z","iopub.execute_input":"2023-09-28T10:00:05.476241Z","iopub.status.idle":"2023-09-28T10:00:05.509508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having outlined the procedure, we will now do create 5 subdataframes and do this in each of those.","metadata":{}},{"cell_type":"code","source":"# First we shuffle train_df_3 by rows\nset.seed(123)\n# Get the number of rows in the dataframe\nn_rows <- nrow(train_df_3)\n# Create a random permutation of row indices\nshuffled_indices <- sample(1:n_rows)\n# Use the shuffled indices to reorder the rows of the dataframe\ntrain_shuff <- train_df_3[shuffled_indices, ]\n\nhead(train_df_3, 3)\nhead(train_shuff, 3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:09.395054Z","iopub.execute_input":"2023-09-28T10:00:09.396363Z","iopub.status.idle":"2023-09-28T10:00:09.551822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train_shuff into 5 smaller dataframes\nn_rows <- nrow(train_shuff)\nrows_per_split <- floor(n_rows / 5)  # Rows per split\nsplit_dataframes <- list()\n\nfor (i in 1:5) {\n  # Calculate the start and end row indices for the current split\n  start_row <- (i - 1) * rows_per_split + 1\n  end_row <- i * rows_per_split\n  \n  # Extract the rows for the current split\n  current_split <- train_shuff[start_row:end_row, ]\n  \n  # Store the current split in the list\n  split_dataframes[[i]] <- current_split\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:12.173197Z","iopub.execute_input":"2023-09-28T10:00:12.174766Z","iopub.status.idle":"2023-09-28T10:00:12.223114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Peak into the splits\nsplit_dataframes[[1]] %>% head(3)\nsplit_dataframes[[2]] %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:15.344870Z","iopub.execute_input":"2023-09-28T10:00:15.346322Z","iopub.status.idle":"2023-09-28T10:00:15.495896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create predictors ranked by Fvalue\nranked_predictors <- function(df) {\n    \n    # Calculate F-values\n    f_values <- c()\n\n    for (pred_index in 2:ncol(df)) {\n        current_f = get_pred_fval(df[[pred_index]], df$activity)\n        f_values[pred_index - 1] = current_f\n    }\n    \n    # Ranked in new dataframe\n    predictor <- names(train_df_3)\n    predictor <- predictor[-1]\n    rank <- seq(1:length(f_values)) ^ 2  # Square to penalize lower rankings\n\n    ranked_df <- data.frame(\n        predictor,\n        f_values\n    )\n\n    sorted_df <- ranked_df[order(-ranked_df$f_values), ]\n    sorted_df$rank = rank\n    sorted_df <- sorted_df %>% select(-f_values)  # not important anymore\n    \n    return(sorted_df) \n}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:20.688796Z","iopub.execute_input":"2023-09-28T10:00:20.690149Z","iopub.status.idle":"2023-09-28T10:00:20.700162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the five rankings\nranked_1 <- ranked_predictors(split_dataframes[[1]])\nranked_2 <- ranked_predictors(split_dataframes[[2]])\nranked_3 <- ranked_predictors(split_dataframes[[3]])\nranked_4 <- ranked_predictors(split_dataframes[[4]])\nranked_5 <- ranked_predictors(split_dataframes[[5]])\n\nhead(ranked_1, 4)  # Note that the top 4 are different already\nhead(ranked_2, 4)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:22.825125Z","iopub.execute_input":"2023-09-28T10:00:22.826592Z","iopub.status.idle":"2023-09-28T10:00:23.895933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join the 5 splits\ntotal_ranking <- left_join(ranked_1, ranked_2, by = \"predictor\") %>%\n                left_join(., ranked_3, by='predictor') %>%\n                left_join(., ranked_4, by='predictor') %>%\n                left_join(., ranked_4, by='predictor')\nsample_n(total_ranking, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:27.650548Z","iopub.execute_input":"2023-09-28T10:00:27.652237Z","iopub.status.idle":"2023-09-28T10:00:27.687997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sum up\ntotal_ranking <- total_ranking %>% mutate(rowsum = rowSums(select(total_ranking, 2:6)))\n\n# arrange by rowsum\ntotal_ranking <- total_ranking %>% \n   arrange(rowsum) %>%\n   mutate(rank = seq(1,length(total_ranking$rowsum)))\n\nhead(total_ranking, 50)\n\n# plot our feature \ntotal_ranking %>%\n  ggplot(aes(x = rank, y = rowsum)) +\n  geom_col()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:47.940507Z","iopub.execute_input":"2023-09-28T10:00:47.941906Z","iopub.status.idle":"2023-09-28T10:00:48.465122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Select Top X features for Training </b>","metadata":{}},{"cell_type":"code","source":"ranked_predictors <- total_ranking %>% select(predictor) %>% pull()  # as vector\n\ntrain_df_top10 <- train_df_3 %>% select(activity, ranked_predictors[1:10])\ntrain_df_top20 <- train_df_3 %>% select(activity, ranked_predictors[1:20])\ntrain_df_top30 <- train_df_3 %>% select(activity, ranked_predictors[1:30])\ntrain_df_top40 <- train_df_3 %>% select(activity, ranked_predictors[1:40])\ntrain_df_top50 <- train_df_3 %>% select(activity, ranked_predictors[1:50])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:56.092581Z","iopub.execute_input":"2023-09-28T10:00:56.094024Z","iopub.status.idle":"2023-09-28T10:00:56.131176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"________________________________________________________________________________________________________\n<h1 id=modelfit><span style=\"font-size: 20px;\"><b> Fitting the models </h1></span></b>\n\nIt's finnaly time to start fitting our models. We are going to fit the following types of models with all our lists of features (e.g. 10, 20, 30, 40, 50).\n\n- Multinomal Logistic Regression\n- K-nearest neighbour\n- Linear Discriminant Analysis\n\nFurthermore we are going to apply k-fold crossvalidation with k = 5 to to all our models to combat issues with overfitting but first we set.seed to be able to have consistent results.","metadata":{"_uuid":"689e96162446707ee9e88a4c229d169f76290b62"}},{"cell_type":"code","source":"# Setting seed before model fitting\nset.seed(1)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:00:59.894225Z","iopub.execute_input":"2023-09-28T10:00:59.895634Z","iopub.status.idle":"2023-09-28T10:00:59.906656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Setting parameters for cross validation </b>\n\nWe set up parameters for useage in the cross validation.","metadata":{}},{"cell_type":"code","source":"# Defining 5-fold cross validation (for all models)\ntrcntr <- trainControl('cv', number = 5, p = 0.8)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:01:03.646270Z","iopub.execute_input":"2023-09-28T10:01:03.649194Z","iopub.status.idle":"2023-09-28T10:01:03.661942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Multinomal Logistic Regression with k-CV </b>\n\nWe fit our Multinomal Logistic Regressions.","metadata":{}},{"cell_type":"code","source":"fit_mlr_10 <- caret::train(activity ~ ., \n                        data = train_df_top10, \n                        method = \"multinom\",\n                        trace = FALSE,\n                        trControl = trcntr)\nfit_mlr_10\n\nfit_mlr_20 <- caret::train(activity ~ ., \n                        data = train_df_top20, \n                        method = \"multinom\",\n                        trace = FALSE,\n                        trControl = trcntr)\nfit_mlr_20\n\nfit_mlr_30 <- caret::train(activity ~ ., \n                        data = train_df_top30, \n                        method = \"multinom\",\n                        trace = FALSE,\n                        trControl = trcntr)\nfit_mlr_30\n\nfit_mlr_40 <- caret::train(activity ~ ., \n                        data = train_df_top40, \n                        method = \"multinom\", \n                        trace = FALSE,\n                        trControl = trcntr)\nfit_mlr_40\n\nfit_mlr_50 <- caret::train(activity ~ ., \n                        data = train_df_top50, \n                        method = \"multinom\", \n                        trace = FALSE,\n                        trControl = trcntr)\nfit_mlr_50","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:01:11.415449Z","iopub.execute_input":"2023-09-28T10:01:11.416880Z","iopub.status.idle":"2023-09-28T10:03:31.571226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> k-NN with k-CV </b>\n\nWe fit our k-nearest neighbour models.","metadata":{}},{"cell_type":"code","source":"fit_knn_10 <- caret::train(activity ~ ., \n                        data = train_df_top10,\n                        preProcess = \"scale\", \n                        method = \"knn\",\n                        trControl = trcntr)\nfit_knn_10\n\n\nfit_knn_20 <- caret::train(activity ~ ., \n                        data = train_df_top20,\n                        preProcess = \"scale\", \n                        method = \"knn\",\n                        trControl = trcntr)\nfit_knn_20\n\nfit_knn_30 <- caret::train(activity ~ ., \n                        data = train_df_top30,\n                        preProcess = \"scale\", \n                        method = \"knn\",\n                        trControl = trcntr)\nfit_knn_30\n\n\nfit_knn_40 <- caret::train(activity ~ ., \n                        data = train_df_top40,\n                        preProcess = \"scale\", \n                        method = \"knn\",\n                        trControl = trcntr)\nfit_knn_40\n\nfit_knn_50 <- caret::train(activity ~ ., \n                        data = train_df_top50,\n                        preProcess = \"scale\", \n                        method = \"knn\",\n                        trControl = trcntr)\nfit_knn_50","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:03:47.803314Z","iopub.execute_input":"2023-09-28T10:03:47.804624Z","iopub.status.idle":"2023-09-28T10:03:59.623593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Linear Discriminant Analysis with k-CV </b>\n\nWe fit our linear discriminant analysis models.","metadata":{}},{"cell_type":"code","source":"fit_lda_10 <- caret::train(activity ~ ., \n                       data = train_df_top10, \n                       method=\"lda\", \n                       trControl = trcntr)\nfit_lda_10\n\n\nfit_lda_20 <- caret::train(activity ~ ., \n                       data = train_df_top20, \n                       method=\"lda\", \n                       trControl = trcntr)\nfit_lda_20\n\nfit_lda_30 <- caret::train(activity ~ ., \n                       data = train_df_top30, \n                       method=\"lda\", \n                       trControl = trcntr)\nfit_lda_30\n\n\nfit_lda_40 <- caret::train(activity ~ ., \n                       data = train_df_top40, \n                       method=\"lda\", \n                       trControl = trcntr)\nfit_lda_40\n\nfit_lda_50 <- caret::train(activity ~ ., \n                       data = train_df_top50, \n                       method=\"lda\", \n                       trControl = trcntr)\nfit_lda_50","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:04:02.289677Z","iopub.execute_input":"2023-09-28T10:04:02.291088Z","iopub.status.idle":"2023-09-28T10:04:04.864914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------\n<h1 id=compmodel><span style=\"font-size: 20px;\"><b> Comparing models </h1></span></b>\n\nWe compare the peformance of the models we just fitted.\n\n**First, we visualize performance with a barplot:**","metadata":{}},{"cell_type":"code","source":"# All resulting models in a list\nmodels <- list(\n    MLR_10 = fit_mlr_10,\n    MLR_20 = fit_mlr_20,\n    MLR_30 = fit_mlr_30,\n    MLR_40 = fit_mlr_40,\n    MLR_50 = fit_mlr_50,\n    \n    kNN_10 = fit_knn_10,\n    kNN_20 = fit_knn_20,\n    kNN_30 = fit_knn_30,\n    kNN_40 = fit_knn_40,\n    kNN_50 = fit_knn_50,\n    \n    LDA_10 = fit_lda_10,\n    LDA_20 = fit_lda_20,\n    LDA_30 = fit_lda_30,\n    LDA_40 = fit_lda_40,\n    LDA_50 = fit_lda_50\n)\n\n# Extracting the cross-validated accuracies from each model\nAcc <- sapply(models, function(mdl) max(mdl$results$Accuracy)) \n\n# Creating a barplot with only the best performing model in red\ncolor <- 1 + (Acc >= max(Acc))\n              \nbarplot(Acc, horiz = TRUE, las = 1, col = color, xlim = c(0, 1))\n              \ncustom_ticks <- seq(0.1, 0.9, by = 0.1)\n              \naxis(1, at = custom_ticks, labels = custom_ticks)\n              \nbest_model_index <- which.max(Acc)\n              \nabline(v = Acc[best_model_index], col = \"red\", lty = 2)\n\n# Adding a title to the graph\ntitle(main = \"Comparing Model Accuracies\")\n\n# Determining the name of the most accurate model and its accuracy percentage\nname_of_model <- names(models)[best_model_index]\naccuracy_percentage <- round(Acc[best_model_index] * 100, 2)\n\n# Creating a subtitle with the model name and accuracy percentage\nsubtitle <- sub <- sprintf(\"The most accurate model is %s with %.2f%% accuracy\",\n                           name_of_model, accuracy_percentage)\n\n# Adding the subtitle to the graph\nmtext(side = 3, text = subtitle, line = 0)\n              \n# Printing accuracy percentages\nprint(Acc*100)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:05:24.004842Z","iopub.execute_input":"2023-09-28T10:05:24.009827Z","iopub.status.idle":"2023-09-28T10:05:24.166149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Second, we test if the performance differences between the models are statistically significant.**\n\nFor this reason we conduct a McNemar test (Chi-square version of the paired t-test, for contingency tables). We will use the pairwise.table function to conduct pairwise comparisons.","metadata":{}},{"cell_type":"code","source":"## Test significance of classifier accuracy differences\n\n# For each model in models compute a predicted label \npreds = sapply(models, predict)  # for each model, a columns of prediction \n\n# For each prediction, test if the prediction is correct\ncorrect = (preds == train_df$activity) # matrix of TRUE and FALSE\n\n# For pairwise.table() we need to define a function that returns the p-values\ncompare_func = function(i,j) mcnemar.test(correct[,i], correct[,j])$p.value\n\n# Compute the table of pairwise comparisons\noptions(digits=3)\nprint(zapsmall(pairwise.table(compare_func, level.names = colnames(preds), \"none\")))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:05:47.263359Z","iopub.execute_input":"2023-09-28T10:05:47.264655Z","iopub.status.idle":"2023-09-28T10:05:51.362534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Selecting the best model </b>\n\nWe select the best model for use in our predictions by hand out of the 15 models we have fitted\n* LDA is generally behind kNN and MLR.\n\n* kNN is flexible, but tend to struggle with higher dimensional data.\n\n* kNN performs similarly to MLR at the top 40 and 50 features, and are statistically different (basd on the McNemar test) besides kNN_40 and MLR_50 (p = 0.085).\n\n* MLR_50 performs slightly better than MLR_40 (89.59173% vs. 89.46948% respectively), but their performance is not significantly different (p = 0.383).\n\n* On the other hand, MLR_40 performs almost a percent higher than MLR_30 (89.46948% vs. 88.36835%) and their performance is significantly different (p < 0.001)\n\n**Therefore fit_mlr_40 is going to be selected as the model that we will use for further predictions.**","metadata":{}},{"cell_type":"code","source":"winner_fit <- fit_mlr_40","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:21:04.375156Z","iopub.execute_input":"2023-09-28T10:21:04.376741Z","iopub.status.idle":"2023-09-28T10:21:04.388151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------\n<h1 id=submissions><span style=\"font-size: 20px;\"><b> Submissions </h1></span></b>\n\n\n<b> Creating Dataframe for Test acceleration features </b>","metadata":{"_uuid":"63f77917e08cd690655137184750cfac494607ce"}},{"cell_type":"code","source":"# Acc features\ntest_filenames <- dir(\"./RawData/Test/\", \"^acc\", full.names = TRUE)\n# map_dfr runs `extractTimeDomainFeatures` on all elements in filenames \n# and binds results row wise\ntest_myData_acc <- map_dfr(test_filenames, extractDomainFeatures, sample_labels) \n\n# Label as Acc feats\ntest_myData_acc <- test_myData_acc %>% rename_with(\n    ~paste0(\"AC_\", .x), 6:length(names(test_myData_acc))\n)\n\n# Check the result\nprint(test_myData_acc)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:21:10.354156Z","iopub.execute_input":"2023-09-28T10:21:10.355907Z","iopub.status.idle":"2023-09-28T10:21:39.309598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Creating Dataframe for Test gyroscope features </b>","metadata":{}},{"cell_type":"code","source":"# Gyro features\ntest_filenames <- dir(\"./RawData/Test/\", \"^gyro\", full.names = TRUE)\ntest_myData_gyro <- map_dfr(test_filenames, extractDomainFeatures, sample_labels) \n\n# Label as Acc feats\ntest_myData_gyro <- test_myData_gyro %>% rename_with(\n    ~paste0(\"GY_\", .x), 6:length(names(test_myData_gyro))\n)\n\n# Check the result\nprint(test_myData_gyro)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:22:48.086162Z","iopub.execute_input":"2023-09-28T10:22:48.089051Z","iopub.status.idle":"2023-09-28T10:23:20.158822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Merging acceleration and gyroscope test features </b>","metadata":{}},{"cell_type":"code","source":"# Merge the two\ntest_df = test_myData_acc %>% \n    left_join(test_myData_gyro,\n              by = c(\"user_id\", \"exp_id\", \"activity\", \"sampleid\", \"epoch\")\n    )\n# Check\ntest_df %>% select(c(1:8), c(85:87)) %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:23:35.485297Z","iopub.execute_input":"2023-09-28T10:23:35.487312Z","iopub.status.idle":"2023-09-28T10:23:35.533368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if other participants are also in there\nunique_users <- unique(test_df$user_id)\nprint(unique_users)\nprint(length(unique_users))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:23:38.230857Z","iopub.execute_input":"2023-09-28T10:23:38.232664Z","iopub.status.idle":"2023-09-28T10:23:38.256312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Predicting Test Dataframe </b>","metadata":{}},{"cell_type":"code","source":"# Predicting based on the winner model fit and training data\npred_df = predict(winner_fit, newdata = test_df, type = \"prob\")\npred_df %>% sample_n(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:23:50.421031Z","iopub.execute_input":"2023-09-28T10:23:50.422635Z","iopub.status.idle":"2023-09-28T10:23:50.494019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding column with highest probability in each row\npred_df$Predicted <- apply(pred_df, 1, function(row) {\n  colnames(pred_df)[which.max(row)]\n})","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:23:54.598756Z","iopub.execute_input":"2023-09-28T10:23:54.600241Z","iopub.status.idle":"2023-09-28T10:23:54.626685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking if it works\npred_df %>% sample_n(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:23:57.479080Z","iopub.execute_input":"2023-09-28T10:23:57.480605Z","iopub.status.idle":"2023-09-28T10:23:57.530648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attaching predictions to test_df\ntest_df = test_df %>% mutate(Predicted = pred_df$Predicted)\n\ntest_df %>% select(activity, Predicted) %>% head(5)  # Makes sense since no labels","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:24:09.674324Z","iopub.execute_input":"2023-09-28T10:24:09.676043Z","iopub.status.idle":"2023-09-28T10:24:09.711718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity check: lets view train_df\ntrain_df %>% select(activity) %>% head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:24:12.854677Z","iopub.execute_input":"2023-09-28T10:24:12.856348Z","iopub.status.idle":"2023-09-28T10:24:12.891457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Formatting the submission file </b>","metadata":{"_uuid":"9d6811f2aa519b89fdd455e046c1201e82109a67","trusted":true}},{"cell_type":"code","source":"test_df %>%\n\n    # prepend \"user\" and \"exp\" to user_id and exp_id\n    mutate(\n        user_id = paste(ifelse(user_id < 10, \"user0\", \"user\"), user_id, sep=\"\"), \n        exp_id = paste(ifelse(exp_id < 10, \"exp0\", \"exp\"), exp_id, sep=\"\")\n    ) %>% \n\n    # unit columnes user_id, exp_id and sample_id into a string \n    # separated by \"_\" and store it in the new variable `Id`\n    unite(Id, user_id, exp_id, sampleid) %>%\n\n    # retain only the `Id` and  predictions\n    select(Id, Predicted = Predicted) %>%\n\n    # write to file\n    write_csv(\"test_set_predictions.csv\")\n\n\n# Check the result: print first 20 lines in the submission file\ncat(readLines(\"test_set_predictions.csv\", 6), sep=\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T10:24:16.434912Z","iopub.execute_input":"2023-09-28T10:24:16.436602Z","iopub.status.idle":"2023-09-28T10:24:16.505290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------\n<h1 id=author><span style=\"font-size: 20px;\"><b> Author Contributions </h1></span></b>\n\nBence: timedom features, spectral features, model fitting, textual edits, overall feature set creation, base feature selection, cleaning up workbook.\n\nJeroen: code styling, html styling, writing of the notebook, found and fixed bug that caused our models to be unable to fit, corrected coding mistake in ANOVA procedure, plotted ranked sum. Committed feature theft.\n\nVincent: cleaning workbook, import gyro data, train_df, test_df, making predictions, spectral features from group 16, removed epochs with < 128 samples, implemented ANOVA feature selection\n\n----------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------\n<h1 id=reference><span style=\"font-size: 20px;\"><b> References </h1></span></b>\n\n* dan_vdmeer, Dave Leitritz, Joost van Kordelaar, Raoul. (2023). BDA 2023 Physical activity recognition. Kaggle. https://kaggle.com/competitions/physical-activity-recognition-bda-2023\n* Grasman, R. (2018). Feature extraction from Signals. Dropbox Paper. Retrieved September 21, 2023, from https://paper.dropbox.com/doc/Feature-extraction-from-Signals-qCp5uvj47gmyuw5nmB8lL\n\n----------------------------------------------------------------------------------------------------------------------------","metadata":{}}]}